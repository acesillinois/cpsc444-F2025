{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhyPgB75TvXpMFRhWNBD+K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xesmaze/cpsc499-sta-fall2024/blob/main/ENSEMBLMethods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Bagging (Random Forest):**\n",
        "\n",
        "  Random Forest is a Bagging method that builds multiple decision trees and aggregates their predictions to improve generalization and reduce overfitting.\n",
        "\n",
        "**2. Boosting (XGBoost):**\n",
        "\n",
        "  XGBoost is a powerful gradient boosting method that builds models sequentially, where each new model attempts to correct the errors made by the previous one.\n",
        "\n",
        "  This example uses the xgboost library with a squared error loss function.\n",
        "\n",
        "**3. Stacking:**\n",
        "\n",
        "  Stacking combines the predictions of several base learners (e.g., Random Forest, SVR) and trains a final meta-learner (e.g., Ridge) to make the final prediction based on the outputs of the base learners.\n",
        "\n",
        "  In this example, a `RandomForestRegressor` and `SVR` are used as base models, and a `Ridge` regression is used as the meta-learner.\n",
        "\n",
        "  For each method, you can compare their respective Mean Squared Errors (MSEs) to understand their effectiveness on the Abalone dataset. The Stacking method typically combines the strengths of its base learners, often outperforming individual models."
      ],
      "metadata": {
        "id": "Jf4tyFEHM0z_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRRRvr0iMuGU",
        "outputId": "07ca9398-0407-48e4-8b19-d187e6a29235"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest MSE: 5.10786495215311\n"
          ]
        }
      ],
      "source": [
        "# Bagging Example with Abalone Dataset\n",
        "# Import necessary libraries\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Abalone dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\"\n",
        "columns = [\"Sex\", \"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\",\n",
        "           \"Viscera weight\", \"Shell weight\", \"Rings\"]\n",
        "data = pd.read_csv(url, names=columns)\n",
        "\n",
        "# One-hot encode the 'Sex' column\n",
        "data = pd.get_dummies(data, columns=[\"Sex\"], drop_first=True)\n",
        "\n",
        "# Split data into features and target\n",
        "X = data.drop(\"Rings\", axis=1)\n",
        "y = data[\"Rings\"]\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest (Bagging example)\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = rf.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Random Forest MSE: {mse}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "# Convert the data to DMatrix format for XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set up parameters for XGBoost (without n_estimators)\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'max_depth': 6,\n",
        "    'learning_rate': 0.1,\n",
        "    'random_state': 42  # Set random seed for reproducibility\n",
        "}\n",
        "\n",
        "# Train the XGBoost model with num_boost_round instead of n_estimators\n",
        "xg_reg = xgb.train(params, dtrain, num_boost_round=100)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = xg_reg.predict(dtest)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"XGBoost MSE: {mse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qB7eq2pPOEpg",
        "outputId": "f3354856-94c4-4238-cc5e-94e6eb631714"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost MSE: 5.10847911780039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Base learners\n",
        "base_learners = [\n",
        "    ('rf', RandomForestRegressor(n_estimators=50, random_state=42)),\n",
        "    ('svr', SVR(kernel='linear')),\n",
        "]\n",
        "\n",
        "# Meta-learner (final model)\n",
        "stack_model = StackingRegressor(\n",
        "    estimators=base_learners,\n",
        "    final_estimator=Ridge()\n",
        ")\n",
        "\n",
        "# Train the Stacking model\n",
        "stack_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = stack_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Stacking Model MSE: {mse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIPM5GuuNdzc",
        "outputId": "12e4894c-d734-423f-c3f8-b0c498f10364"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Model MSE: 4.8071347773329\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pruning Example with Early Stopping at XGBoost**\n",
        "\n",
        "Pruning in boosting algorithms like XGBoost usually refers to early stopping, where the training process halts if the performance on a validation set doesn't improve after a certain number of rounds (boosting iterations).\n",
        "\n",
        " We'll add early stopping as the pruning mechanism in XGBoost, where training stops if no improvement is seen in the validation error for a defined number of rounds."
      ],
      "metadata": {
        "id": "_PaI__fYOZ2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the Abalone dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\"\n",
        "columns = [\"Sex\", \"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\",\n",
        "           \"Viscera weight\", \"Shell weight\", \"Rings\"]\n",
        "data = pd.read_csv(url, names=columns)\n",
        "\n",
        "# One-hot encode the 'Sex' column\n",
        "data = pd.get_dummies(data, columns=[\"Sex\"], drop_first=True)\n",
        "\n",
        "# Split data into features and target\n",
        "X = data.drop(\"Rings\", axis=1)\n",
        "y = data[\"Rings\"]\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Convert data to DMatrix format for XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dval = xgb.DMatrix(X_val, label=y_val)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set up parameters for XGBoost (without n_estimators)\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'max_depth': 6,\n",
        "    'learning_rate': 0.1,\n",
        "    'random_state': 42  # Set random seed for reproducibility\n",
        "}\n",
        "\n",
        "# Train the XGBoost model with early stopping for pruning\n",
        "xg_reg = xgb.train(params,\n",
        "                   dtrain,\n",
        "                   num_boost_round=100,          # Maximum number of boosting rounds\n",
        "                   evals=[(dval, 'validation')], # Validation set to monitor\n",
        "                   early_stopping_rounds=20,     # Stop if no improvement in 20 rounds\n",
        "                   verbose_eval=True)            # Print progress\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = xg_reg.predict(dtest)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"XGBoost with Early Stopping MSE: {mse}\")\n",
        "print(f\"XGBoost with Early Stopping RMSE: {rmse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTa_CYIDOqig",
        "outputId": "a14848e5-b19f-47a0-a521-b4f3d38ac1a8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\tvalidation-rmse:3.00877\n",
            "[1]\tvalidation-rmse:2.87303\n",
            "[2]\tvalidation-rmse:2.75336\n",
            "[3]\tvalidation-rmse:2.65315\n",
            "[4]\tvalidation-rmse:2.56681\n",
            "[5]\tvalidation-rmse:2.50281\n",
            "[6]\tvalidation-rmse:2.45023\n",
            "[7]\tvalidation-rmse:2.40445\n",
            "[8]\tvalidation-rmse:2.37101\n",
            "[9]\tvalidation-rmse:2.33862\n",
            "[10]\tvalidation-rmse:2.32155\n",
            "[11]\tvalidation-rmse:2.30271\n",
            "[12]\tvalidation-rmse:2.29208\n",
            "[13]\tvalidation-rmse:2.28186\n",
            "[14]\tvalidation-rmse:2.27022\n",
            "[15]\tvalidation-rmse:2.26831\n",
            "[16]\tvalidation-rmse:2.26045\n",
            "[17]\tvalidation-rmse:2.25583\n",
            "[18]\tvalidation-rmse:2.25162\n",
            "[19]\tvalidation-rmse:2.25121\n",
            "[20]\tvalidation-rmse:2.24850\n",
            "[21]\tvalidation-rmse:2.24659\n",
            "[22]\tvalidation-rmse:2.24914\n",
            "[23]\tvalidation-rmse:2.24994\n",
            "[24]\tvalidation-rmse:2.24872\n",
            "[25]\tvalidation-rmse:2.25090\n",
            "[26]\tvalidation-rmse:2.24861\n",
            "[27]\tvalidation-rmse:2.24767\n",
            "[28]\tvalidation-rmse:2.24974\n",
            "[29]\tvalidation-rmse:2.24912\n",
            "[30]\tvalidation-rmse:2.25346\n",
            "[31]\tvalidation-rmse:2.25459\n",
            "[32]\tvalidation-rmse:2.25063\n",
            "[33]\tvalidation-rmse:2.25453\n",
            "[34]\tvalidation-rmse:2.25443\n",
            "[35]\tvalidation-rmse:2.25533\n",
            "[36]\tvalidation-rmse:2.25716\n",
            "[37]\tvalidation-rmse:2.25636\n",
            "[38]\tvalidation-rmse:2.25651\n",
            "[39]\tvalidation-rmse:2.25711\n",
            "[40]\tvalidation-rmse:2.25573\n",
            "XGBoost with Early Stopping MSE: 4.46763098597263\n",
            "XGBoost with Early Stopping RMSE: 2.2665877381175985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What happens when you increase the maximum number of early stopping rounds to 10 and boosting rounds to 500?**"
      ],
      "metadata": {
        "id": "bQ4_2FaxPxP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Load the Abalone dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\"\n",
        "columns = [\"Sex\", \"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\",\n",
        "           \"Viscera weight\", \"Shell weight\", \"Rings\"]\n",
        "data = pd.read_csv(url, names=columns)\n",
        "\n",
        "# One-hot encode the 'Sex' column\n",
        "data = pd.get_dummies(data, columns=[\"Sex\"], drop_first=True)\n",
        "\n",
        "# Split data into features and target\n",
        "X = data.drop(\"Rings\", axis=1)\n",
        "y = data[\"Rings\"]\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Convert data to DMatrix format for XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dval = xgb.DMatrix(X_val, label=y_val)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set up parameters for XGBoost (without n_estimators)\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'max_depth': 6,\n",
        "    'learning_rate': 0.1,\n",
        "}\n",
        "\n",
        "# Train the XGBoost model with early stopping for pruning\n",
        "xg_reg = xgb.train(params,\n",
        "                   dtrain,\n",
        "                   num_boost_round=500,          # Maximum number of boosting rounds\n",
        "                   evals=[(dval, 'validation')], # Validation set to monitor\n",
        "                   early_stopping_rounds=10,     # Stop if no improvement in 20 rounds\n",
        "                   verbose_eval=True)            # Print progress\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = xg_reg.predict(dtest)\n",
        "\n",
        "# Compute RMSE for the final evaluation\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(f\"XGBoost with Early Stopping MSE: {mse}\")\n",
        "print(f\"XGBoost with Early Stopping RMSE: {rmse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFNwm9PLPG3W",
        "outputId": "6a332d20-ccad-4de2-c68e-714507af3de6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\tvalidation-rmse:3.00877\n",
            "[1]\tvalidation-rmse:2.87303\n",
            "[2]\tvalidation-rmse:2.75336\n",
            "[3]\tvalidation-rmse:2.65315\n",
            "[4]\tvalidation-rmse:2.56681\n",
            "[5]\tvalidation-rmse:2.50281\n",
            "[6]\tvalidation-rmse:2.45023\n",
            "[7]\tvalidation-rmse:2.40445\n",
            "[8]\tvalidation-rmse:2.37101\n",
            "[9]\tvalidation-rmse:2.33862\n",
            "[10]\tvalidation-rmse:2.32155\n",
            "[11]\tvalidation-rmse:2.30271\n",
            "[12]\tvalidation-rmse:2.29208\n",
            "[13]\tvalidation-rmse:2.28186\n",
            "[14]\tvalidation-rmse:2.27022\n",
            "[15]\tvalidation-rmse:2.26831\n",
            "[16]\tvalidation-rmse:2.26045\n",
            "[17]\tvalidation-rmse:2.25583\n",
            "[18]\tvalidation-rmse:2.25162\n",
            "[19]\tvalidation-rmse:2.25121\n",
            "[20]\tvalidation-rmse:2.24850\n",
            "[21]\tvalidation-rmse:2.24659\n",
            "[22]\tvalidation-rmse:2.24914\n",
            "[23]\tvalidation-rmse:2.24994\n",
            "[24]\tvalidation-rmse:2.24872\n",
            "[25]\tvalidation-rmse:2.25090\n",
            "[26]\tvalidation-rmse:2.24861\n",
            "[27]\tvalidation-rmse:2.24767\n",
            "[28]\tvalidation-rmse:2.24974\n",
            "[29]\tvalidation-rmse:2.24912\n",
            "[30]\tvalidation-rmse:2.25346\n",
            "XGBoost with Early Stopping MSE: 4.46763098597263\n",
            "XGBoost with Early Stopping RMSE: 2.120969891706594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lets investigate how changing the number of boost rounds effect the loss?"
      ],
      "metadata": {
        "id": "iWvMyjjbVlBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import plotly.graph_objs as go\n",
        "from plotly.subplots import make_subplots\n",
        "import pandas as pd\n",
        "\n",
        "# Function to calculate RMSE for different boosting rounds with fixed seed\n",
        "def get_rmse(num_boost_rounds, dtrain, dtest, y_test):\n",
        "    # Train the XGBoost model with a fixed random seed for reproducibility\n",
        "    xg_reg = xgb.train(params, dtrain, num_boost_round=num_boost_rounds)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = xg_reg.predict(dtest)\n",
        "\n",
        "    # Compute RMSE\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    return rmse\n",
        "\n",
        "# Number of boosting rounds to evaluate\n",
        "boost_rounds = [20,30,40,50,100, 200, 300, 400, 500]\n",
        "rmse_results = []\n",
        "\n",
        "# Convert the data to DMatrix format for XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set up parameters for XGBoost with a fixed random seed\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'max_depth': 6,\n",
        "    'learning_rate': 0.1,\n",
        "    'random_state': 42  # Set random seed for reproducibility\n",
        "}\n",
        "\n",
        "# Calculate RMSE for each number of boosting rounds\n",
        "for num_round in boost_rounds:\n",
        "    rmse = get_rmse(num_round, dtrain, dtest, y_test)\n",
        "    rmse_results.append(rmse)\n",
        "\n",
        "# Prepare data for Plotly\n",
        "data = pd.DataFrame({'Boosting Rounds': boost_rounds, 'RMSE': rmse_results})\n",
        "\n",
        "# Create Plotly figure\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add line plot for RMSE\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=data['Boosting Rounds'],\n",
        "    y=data['RMSE'],\n",
        "    mode='lines+markers',\n",
        "    name='RMSE'\n",
        "))\n",
        "\n",
        "# Customize the layout\n",
        "fig.update_layout(\n",
        "    title='XGBoost RMSE vs Number of Boosting Rounds',\n",
        "    xaxis_title='Number of Boosting Rounds',\n",
        "    yaxis_title='Root Mean Squared Error (RMSE)',\n",
        "    template='plotly'\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "N9SuR_JgSAPz",
        "outputId": "5a41eb52-9ed7-4424-87ad-8b1134f8ff55"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"b397909d-389a-4c65-ba00-5354e7db5b29\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"b397909d-389a-4c65-ba00-5354e7db5b29\")) {                    Plotly.newPlot(                        \"b397909d-389a-4c65-ba00-5354e7db5b29\",                        [{\"mode\":\"lines+markers\",\"name\":\"RMSE\",\"x\":[20,30,40,50,100,200,300,400,500],\"y\":[2.139145905076646,2.1213352478929255,2.111787911944483,2.115927463525861,2.152835189498734,2.2046376778236367,2.229869439529413,2.251887621169686,2.2665877381175985],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"XGBoost RMSE vs Number of Boosting Rounds\"},\"xaxis\":{\"title\":{\"text\":\"Number of Boosting Rounds\"}},\"yaxis\":{\"title\":{\"text\":\"Root Mean Squared Error (RMSE)\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('b397909d-389a-4c65-ba00-5354e7db5b29');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}